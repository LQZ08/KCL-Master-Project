{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMsQzYsEKE8f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI8BbqZZKE8i",
        "outputId": "4aef8732-09ab-401f-cc88-ce4467c6c1d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=8ecce6ec5c9e33647ae9d9894e745cc8091a147b2505dafeee323144d713cb03\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ],
      "source": [
        "# Author: Qingzhou Li and Leo Zhang based on https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html by Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install seqeval\n",
        "from seqeval.metrics import f1_score, classification_report\n",
        "torch.manual_seed(1)\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsRtInXTKE8i"
      },
      "source": [
        "Helper functions to make the code more readable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdEPczmrKE8i"
      },
      "outputs": [],
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "\n",
        "# (This is for CRF model) Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bi-LSTM for NER (Toy sample)"
      ],
      "metadata": {
        "id": "_3bNdMPtfsSE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd4TLku1KE8j"
      },
      "source": [
        "Create model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KvK4fxRKE8j"
      },
      "outputs": [],
      "source": [
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        #self.char_embeds = nn.xxxxxxxx\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "\n",
        "\n",
        "    def cross_entropy_loss(self, sentence, tags):\n",
        "        feats = self.forward(sentence)\n",
        "        loss_fuction = nn.CrossEntropyLoss()\n",
        "        loss = loss_fuction(feats, tags)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "\n",
        "        return lstm_feats\n",
        "\n",
        "    def predict(self, sentence):\n",
        "        ix_pred = np.argmax(self.forward(sentence).numpy(), axis=1)\n",
        "        ix_to_tag = {v:k for k, v in self.tag_to_ix.items()}\n",
        "        tag_pred = [ix_to_tag[i] for i in ix_pred]\n",
        "        return tag_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the trainning data\n",
        "training_entity = pd.read_csv('entity.csv')\n",
        "with open(\"sents_dict.txt\", \"r\") as fp:\n",
        "  sents_dict = json.load(fp)"
      ],
      "metadata": {
        "id": "T_c_MX3x-IG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the trainning data with IOB tagging\n",
        "training_data1 = []\n",
        "for key in sents_dict:\n",
        "  iob = [\"O\"] * len(sents_dict[key].split())\n",
        "  current_table = training_entity[training_entity[\"sent_num\"] == key]\n",
        "  for index, row in current_table.iterrows():\n",
        "    start = row['start']\n",
        "    end = row['end']\n",
        "    concept = row['concept']\n",
        "    if start == (end - 1):\n",
        "      iob = iob[:start] + ['B-'+concept] + iob[end:]\n",
        "    else:\n",
        "      num_i = end - start - 1\n",
        "      iob = iob[:start] + ['B-'+concept] + ['I-'+concept] * num_i + iob[end:]\n",
        "  training_data1.append((key,sents_dict[key].split(),iob))"
      ],
      "metadata": {
        "id": "pI7wLfznb1D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the test data\n",
        "test_entity = pd.read_csv('test_entity.csv')\n",
        "with open(\"test_sents_dict.txt\", \"r\") as fp:\n",
        "  test_sents_dict = json.load(fp)"
      ],
      "metadata": {
        "id": "jy2zJpE1hX9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format the test data with IOB tagging\n",
        "test_data = []\n",
        "for key in test_sents_dict:\n",
        "  iob = [\"O\"] * len(test_sents_dict[key].split())\n",
        "  current_table = test_entity[test_entity[\"sent_num\"] == key]\n",
        "  for index, row in current_table.iterrows():\n",
        "    start = row['start']\n",
        "    end = row['end']\n",
        "    concept = row['concept']\n",
        "    if start == (end - 1):\n",
        "      iob = iob[:start] + ['B-'+concept] + iob[end:]\n",
        "    else:\n",
        "      num_i = end - start - 1\n",
        "      iob = iob[:start] + ['B-'+concept] + ['I-'+concept] * num_i + iob[end:]\n",
        "  test_data.append((key,test_sents_dict[key].split(),iob))"
      ],
      "metadata": {
        "id": "xTXkuMP6CD8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split training data into train and validation 80:20\n",
        "train, val = train_test_split(training_data1, test_size=0.2, random_state=1234)"
      ],
      "metadata": {
        "id": "6dlhbV9YFcFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, tolerance=5, min_delta=0):\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.tolerance:\n",
        "                return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "iT8LhptsbMi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cDGPMkKE8j"
      },
      "source": [
        "Run training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOVPC5OdKE8j",
        "outputId": "77cdfcc8-b6d6-4d24-be27-fa9c61442663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label tensor([ 0.,  3., 12.,  9., 18.,  4.])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 16/150 [05:16<44:11, 19.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are at epoch: 17\n",
            "prediction [ 0  3 12  9 18  4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "training_data = train\n",
        "validation_data = val\n",
        "word_to_ix = {}\n",
        "for key, sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "for key, sentence, tags in validation_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "for key, sentence, tags in test_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "tag_to_ix = {\"O\": 0, START_TAG: 1, STOP_TAG: 2, \"B-Drug\": 3, \"B-Route\": 4, \"B-Reason\": 5, \"B-Duration\": 6, \"B-Dosage\": 7, \"B-Frequency\": 8, \"B-Strength\": 9,\n",
        "             \"B-Form\": 10, \"B-ADE\": 11, \"I-Drug\": 12, \"I-Route\": 13, \"I-Reason\": 14, \"I-Duration\": 15, \"I-Dosage\": 16, \"I-Frequency\": 17,\n",
        "             \"I-Strength\": 18, \"I-Form\": 19, \"I-ADE\": 20\n",
        "             }\n",
        "model = BiLSTM( len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)\n",
        "early_stopping = EarlyStopping(tolerance=5, min_delta=0)\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "epoch_i = 0\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[1][1], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[1][2]], dtype=torch.float)\n",
        "    print('label',precheck_tags)\n",
        "\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for i in tqdm(range(150)):\n",
        "    for key, sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.cross_entropy_loss(sentence_in, targets)\n",
        "\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # split into training and validation, remember the loss in validation find the lowest validation loss, early stop.\n",
        "    # performance measure on validation data.\n",
        "    train_loss.append(loss.item())\n",
        "    with torch.no_grad():\n",
        "      for key, sentence, tags in validation_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long, requires_grad=False)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        val_loss = model.cross_entropy_loss(sentence_in, targets)\n",
        "\n",
        "    epoch_i += 1\n",
        "    # early stopping\n",
        "    if early_stopping.early_stop(val_loss):\n",
        "      print(\"We are at epoch:\", epoch_i)\n",
        "      break\n",
        "    validation_loss.append(val_loss.item())\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  precheck_sent = prepare_sequence(training_data[1][1], word_to_ix)\n",
        "  print('prediction', np.argmax(model(precheck_sent).numpy(), axis = 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual NER model performance measure\n",
        "pred = []\n",
        "true = []\n",
        "for i in test_data:\n",
        "  with torch.no_grad():\n",
        "    prepare_sent = prepare_sequence(i[1], word_to_ix)\n",
        "    y_pred = model.predict(sentence= prepare_sent)\n",
        "    pred.append(y_pred)\n",
        "    true.append(i[2])\n",
        "\n",
        "print(f1_score(true,pred))\n",
        "print(classification_report(true, pred))"
      ],
      "metadata": {
        "id": "BVrbOsF3f8SI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b34326-12d4-4860-c685-d9a7b90b0e01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7942392566782811\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ADE       0.30      0.07      0.11       625\n",
            "      Dosage       0.87      0.80      0.84      2681\n",
            "        Drug       0.82      0.81      0.81     10575\n",
            "    Duration       0.57      0.50      0.53       378\n",
            "        Form       0.92      0.87      0.89      4354\n",
            "   Frequency       0.82      0.74      0.78      4012\n",
            "      Reason       0.52      0.54      0.53      2545\n",
            "       Route       0.89      0.89      0.89      3513\n",
            "    Strength       0.78      0.81      0.79      4230\n",
            "\n",
            "   micro avg       0.81      0.78      0.79     32913\n",
            "   macro avg       0.72      0.67      0.69     32913\n",
            "weighted avg       0.80      0.78      0.79     32913\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def list_duplicates(seq):\n",
        "    tally = defaultdict(list)\n",
        "    for i,item in enumerate(seq):\n",
        "        tally[item].append(i)\n",
        "    return ((key,locs) for key,locs in tally.items() if len(locs)>=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "yJhQLhPnBaxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify predicted entities with their respective position in a sentence\n",
        "out = []\n",
        "for key, sentence, tags in test_data:\n",
        "  with torch.no_grad():\n",
        "    prepare_sent = prepare_sequence(sentence, word_to_ix)\n",
        "    pred_entity = model.predict(sentence= prepare_sent)\n",
        "    relation = [key,sentence]\n",
        "    sample = []\n",
        "    indices = [i for i, x in enumerate(pred_entity) if x == \"B-Drug\"]\n",
        "    for i in indices:\n",
        "      sec_index = i+1\n",
        "      if i != (len(pred_entity)-1):\n",
        "        while sec_index < len(pred_entity) and pred_entity[sec_index] == 'I-Drug':\n",
        "          sec_index += 1\n",
        "      drug_index = [i,sec_index]\n",
        "\n",
        "      for item in sorted(list_duplicates(pred_entity)):\n",
        "        if item[0][0] == 'B' and item[0] != 'B-Drug':\n",
        "          for i in item[1]:\n",
        "            seco_index = i+1\n",
        "            if i != (len(pred_entity)-1):\n",
        "              while seco_index < len(pred_entity) and pred_entity[seco_index] == ''.join(['I-',item[0][2:]]):\n",
        "                seco_index += 1\n",
        "            entity_index = [i,seco_index]\n",
        "            sample.append([entity_index, drug_index, item[0][2:]+'-'+'Drug'])\n",
        "    relation.append(sample)\n",
        "    out.append(relation)"
      ],
      "metadata": {
        "id": "-Wx4qr9R9LgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the gold standard test data\n",
        "with open(\"test_relation.txt\", \"r\") as fp:\n",
        "  test_relation = json.load(fp)"
      ],
      "metadata": {
        "id": "boRvghf5pGrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify TP, FP, FN in the relations formed from predicted entities\n",
        "TP = []\n",
        "FP = []\n",
        "FN = []\n",
        "for key1, sentence1, relation1 in out:\n",
        "  for key2, sentence2, relation2 in test_relation:\n",
        "    if key1 == key2 and sentence1 == sentence2:\n",
        "      for i in relation1:\n",
        "        if i not in relation2:\n",
        "          FP.append(i)\n",
        "        elif i in relation2:\n",
        "          TP.append(i)\n",
        "      for i in relation2:\n",
        "        if i not in relation1:\n",
        "          FN.append(i)"
      ],
      "metadata": {
        "id": "DzmrS_fJHRF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(TP))\n",
        "print(len(FP))\n",
        "print(len(FN))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKu1urACIRsf",
        "outputId": "94b52a03-b86b-47d3-f44b-325dbb4ac5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15158\n",
            "31435\n",
            "6920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relation_out = []\n",
        "for key1, sentence1, relation1 in out:\n",
        "  rela = []\n",
        "  for key2, sentence2, relation2 in test_relation:\n",
        "    if key1 == key2 and sentence1 == sentence2:\n",
        "      for i in relation1:\n",
        "        if i in relation2:\n",
        "          rela.append(i)\n",
        "      relation_out.append([key1,sentence1,rela])"
      ],
      "metadata": {
        "id": "-qAX6PBLbHCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(relation_out[0:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbMEBWgvb2m-",
        "outputId": "40488e64-1a7e-48e2-c348-c13ed80cd436"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['0-1', ['He', 'received', '2mg', 'IV', 'ativan', 'with', 'somnolence', 'and', 'apenea'], [[[6, 7], [4, 5], 'ADE-Drug'], [[3, 4], [4, 5], 'Route-Drug'], [[2, 3], [4, 5], 'Strength-Drug']]], ['0-6', ['Patient', 'developed', 'SVT', 'in', 'the', 'cath', 'lab', 'and', 'was', 'treated', 'with', '9', 'mg', 'IV', 'metoprolol'], [[[13, 14], [14, 15], 'Route-Drug'], [[11, 13], [14, 15], 'Strength-Drug']]], ['0-11', ['Evidence', 'of', 'right', 'heart', 'strain,', 'manifest', 'as', 'reflux', 'of', 'IV', 'contrast'], [[[9, 10], [10, 11], 'Route-Drug']]], ['0-15', ['He', 'was', 'started', 'on', 'heparin', 'ggt', 'in', 'house', 'and', 'transitioned', 'to', 'an', 'anticoagulation', 'regimen', 'of', 'coumadin', ',', 'with', 'a', 'lovenox'], [[[5, 6], [4, 5], 'Route-Drug']]], ['0-22', ['He', 'was', 'maintained', 'on', 'a', 'valium', 'CIWA', 'and', 'klonipin', '1mg', 'TID'], [[[10, 11], [8, 9], 'Frequency-Drug'], [[9, 10], [8, 9], 'Strength-Drug']]], ['0-27', ['Psych', 'was', 'consulted', 'who', 'felt', 'that', 'this', 'was', 'less', 'likely', 'to', 'be', 'withdrawal', 'and', 'instead', 'a', 'behavioral', 'issue', '/', 'delerium', ',', 'likely', 'in', 'setting', 'of', 'supratherapeutic', 'valproate'], [[[19, 20], [26, 27], 'ADE-Drug']]], ['0-30', ['They', 'recommended', 'haldol', 'for', 'his', 'agitation', 'which', 'was', 'continued', '5mg', 'PRN', 'along', 'with', '10mg', 'QHS'], [[[14, 15], [2, 3], 'Frequency-Drug'], [[5, 6], [2, 3], 'Reason-Drug'], [[9, 10], [2, 3], 'Strength-Drug'], [[13, 14], [2, 3], 'Strength-Drug']]], ['0-43', ['Increased', 'metoprolol', 'to', '75', 'mg', 'po', 'qid', 'standing', 'which', 'has', 'controlled', 'rate', 'well,', 'though', 'still', 'with', 'frequent', 'episodes', 'of', 'rapid', 'ventricular', 'response', 'that', 'respond', 'to', 'IV', 'lopressor', '5', 'mg', 'x', '1'], [[[30, 31], [26, 27], 'Dosage-Drug'], [[25, 26], [26, 27], 'Route-Drug'], [[27, 29], [26, 27], 'Strength-Drug']]], ['0-52', ['Because', 'of', 'continued', 'episodes', 'of', 'RVR', '(likely', 'in', 'the', 'setting', 'of', 'pericardial', 'effusion', 'and', 'multiple', 'PEs)', 'pt', 'started', 'on', 'diltiazem', 'drip', ',', 'weaned', 'off', 'and', 'started', 'on', 'diltiazem', 'ER', '120', 'mg', 'daily', ',', 'and', 'metoprolol', 'changed', 'to', '200', 'mg', 'of', 'metoprolol', 'succinate'], [[[20, 21], [19, 20], 'Route-Drug'], [[37, 39], [40, 42], 'Strength-Drug']]], ['0-62', ['He', 'was', 'continued', 'home', 'AEDs.', 'Depakote', 'ER', '1250/1500/1500', 'and', 'Oxcarbazepine', '600', 'mg', '[**Hospital1', '**]', 'which', 'was', 'often', 'given', 'IV'], []]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"re_test\", \"wb\") as fp:\n",
        "   pickle.dump(relation_out, fp)"
      ],
      "metadata": {
        "id": "XYZnGrNTVx1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Format end-to-end relations for RE model testing\n",
        "output_relation = []\n",
        "for i in relation_out:\n",
        "  output_relation.append(('re',i))"
      ],
      "metadata": {
        "id": "sO0-eWwxh3l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "Baseline: SGD, lr=0.01, hidden dim=4, tolerance=5\n",
        "\n",
        "baseline: weighted avg f1: 0.59 117 epochs\n",
        "\n",
        "SGD, lr=0.01, hidden dim=64, tolerance=5: 0.66 32 epochs\n",
        "                       0.80 28 epochs\n",
        "                       0.68 42\n",
        "                       0.70 51\n",
        "                       0.68 41\n",
        "\n",
        "SGD, lr=0.01, hidden dim=4, tolerance=5: 0.67 37 epochs\n",
        "                       0.66 38 epochs\n",
        "                       0.50 57\n",
        "                       0.42 37\n",
        "\n",
        "SGD, lr=0.01, hidden dim=4, tolerance=10: 0.75 83 epochs\n",
        "                       0.74 78 epochs\n",
        "\n",
        "SGD, lr=0.1, hidden dim=64, tolerance=5: 0.82 38 epochs\n",
        "\n",
        "SGD, lr=0.2, hidden dim=64, tolerance=5: 0.82 22 epochs\n",
        "\n",
        "SGD, lr=0.3, hidden dim=64, tolerance=5: 0.83 16 epochs\n",
        "\n",
        "SGD, lr=0.3, hidden dim=64, tolerance=8: 0.84 24 epochs\n",
        "\n",
        "SGD, lr=0.4, hidden dim=64, tolerance=5: 0.82 13 epochs\n",
        "\n",
        "SGD, lr=0.01, hidden dim=128, tolerance=5: 0.76 12 epochs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K4fWsVKDw2Ud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BiLSTM + CRF for NER\n",
        "\n"
      ],
      "metadata": {
        "id": "Bqzl0rCKfiIs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gepbB2gPKE8h"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "For this section, we will see a full, complicated example of a Bi-LSTM\n",
        "Conditional Random Field for named-entity recognition. The LSTM tagger\n",
        "above is typically sufficient for part-of-speech tagging, but a sequence\n",
        "model like the CRF is really essential for strong performance on NER.\n",
        "Familiarity with CRF's is assumed. Although this name sounds scary, all\n",
        "the model is a CRF but where an LSTM provides the features. This is\n",
        "an advanced model though, far more complicated than any earlier model in\n",
        "this tutorial. If you want to skip it, that is fine. To see if you're\n",
        "ready, see if you can:\n",
        "\n",
        "-  Write the recurrence for the viterbi variable at step i for tag k.\n",
        "-  Modify the above recurrence to compute the forward variables instead.\n",
        "-  Modify again the above recurrence to compute the forward variables in\n",
        "   log-space (hint: log-sum-exp)\n",
        "\n",
        "If you can do those three things, you should be able to understand the\n",
        "code below. Recall that the CRF computes a conditional probability. Let\n",
        "$y$ be a tag sequence and $x$ an input sequence of words.\n",
        "Then we compute\n",
        "\n",
        "\\begin{align}P(y|x) = \\frac{\\exp{(\\text{Score}(x, y)})}{\\sum_{y'} \\exp{(\\text{Score}(x, y')})}\\end{align}\n",
        "\n",
        "Where the score is determined by defining some log potentials\n",
        "$\\log \\psi_i(x,y)$ such that\n",
        "\n",
        "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_i(x,y)\\end{align}\n",
        "\n",
        "To make the partition function tractable, the potentials must look only\n",
        "at local features.\n",
        "\n",
        "In the Bi-LSTM CRF, we define two kinds of potentials: emission and\n",
        "transition. The emission potential for the word at index $i$ comes\n",
        "from the hidden state of the Bi-LSTM at timestep $i$. The\n",
        "transition scores are stored in a $|T|x|T|$ matrix\n",
        "$\\textbf{P}$, where $T$ is the tag set. In my\n",
        "implementation, $\\textbf{P}_{j,k}$ is the score of transitioning\n",
        "to tag $j$ from tag $k$. So:\n",
        "\n",
        "\\begin{align}\\text{Score}(x,y) = \\sum_i \\log \\psi_\\text{EMIT}(y_i \\rightarrow x_i) + \\log \\psi_\\text{TRANS}(y_{i-1} \\rightarrow y_i)\\end{align}\n",
        "\n",
        "\\begin{align}= \\sum_i h_i[y_i] + \\textbf{P}_{y_i, y_{i-1}}\\end{align}\n",
        "\n",
        "where in this second expression, we think of the tags as being assigned\n",
        "unique non-negative indices.\n",
        "\n",
        "If the above discussion was too brief, you can check out\n",
        "[this](http://www.cs.columbia.edu/%7Emcollins/crf.pdf)_ write up from\n",
        "Michael Collins on CRFs.\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "The example below implements the forward algorithm in log space to\n",
        "compute the partition function, and the viterbi algorithm to decode.\n",
        "Backpropagation will compute the gradients automatically for us. We\n",
        "don't have to do anything by hand.\n",
        "\n",
        "The implementation is not optimized. If you understand what is going on,\n",
        "you'll probably quickly see that iterating over the next tag in the\n",
        "forward algorithm could probably be done in one big operation. I wanted\n",
        "to code to be more readable. If you want to make the relevant change,\n",
        "you could probably use this tagger for real tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model"
      ],
      "metadata": {
        "id": "m94K9aG4fy6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "\n",
        "        # Find the best path, given the features.\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "metadata": {
        "id": "SltvI0yIYrMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running Training"
      ],
      "metadata": {
        "id": "xj00dyirf1CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 4\n",
        "\n",
        "# Make up some training data\n",
        "training_data = [(\n",
        "    \"the wall street journal reported today that apple corporation made money\".split(),\n",
        "    \"B I I I O O O B I O O\".split()\n",
        "), (\n",
        "    \"georgia tech is a university in georgia\".split(),\n",
        "    \"B I O O O O B\".split()\n",
        ")]\n",
        "\n",
        "word_to_ix = {}\n",
        "for sentence, tags in training_data:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
        "    print(model(precheck_sent))\n",
        "\n",
        "# Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for epoch in range(\n",
        "        300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is,\n",
        "        # turn them into Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        # calling optimizer.step()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Check predictions after training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
        "    print(model(precheck_sent))\n",
        "# We got it!"
      ],
      "metadata": {
        "id": "CTldBHmea-q3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}