{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMsQzYsEKE8f"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI8BbqZZKE8i"
      },
      "outputs": [],
      "source": [
        "# Author: Qingzhou Li and Leo Zhang based on https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html by Robert Guthrie\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import json\n",
        "torch.manual_seed(1)\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsRtInXTKE8i"
      },
      "source": [
        "Helper functions to make the code more readable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdEPczmrKE8i"
      },
      "outputs": [],
      "source": [
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bi-LSTM for RE (Toy sample)"
      ],
      "metadata": {
        "id": "_3bNdMPtfsSE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bd4TLku1KE8j"
      },
      "source": [
        "Create model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KvK4fxRKE8j"
      },
      "outputs": [],
      "source": [
        "# We will first use bi-LSTM to get contextual embeddings for each token. Then we get entity embeddings by averaging its constituting token embeddings. Then we concat\n",
        "# head and tail embeddigns. We make predictions based on the concat embedding.\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, relation_to_ix, embedding_dim, hidden_dim):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.relation_to_ix = relation_to_ix\n",
        "        self.relation_size = len(relation_to_ix)\n",
        "        #self.char_embeds = nn.xxxxxxxx\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)  #embedding words\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=1, bidirectional=True)    #contextual embeddings for each tokens using bi-LSTM\n",
        "\n",
        "        # Maps the output of the concat embedding into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.relation_size)\n",
        "\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
        "                torch.randn(2, 1, self.hidden_dim // 2))\n",
        "\n",
        "\n",
        "\n",
        "    def cross_entropy_loss(self, sentence, head, tail, tags):\n",
        "        feats = self.forward(sentence,head,tail)\n",
        "        loss_fuction = nn.CrossEntropyLoss()\n",
        "        loss = loss_fuction(feats, tags)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def forward(self, sentence, head, tail):\n",
        "        # Get embeddings for each tokens\n",
        "        self.hidden = self.init_hidden()\n",
        "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
        "        #now we get embeddings for head entity and tail entity by averaging embeddigns of constituting tokens.\n",
        "        lstm_head = torch.mean(lstm_out[head[0]:head[1]],dim=0)\n",
        "        lstm_tail = torch.mean(lstm_out[tail[0]:tail[1]],dim=0)\n",
        "        #now we concat head and tail embeddings\n",
        "        lstm_entities = torch.cat([lstm_head,lstm_tail])\n",
        "        #then we predict relations based on concat embeddings\n",
        "        lstm_feats = self.hidden2tag(lstm_entities)\n",
        "        return lstm_feats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the trainning data\n",
        "with open(\"relation.txt\", \"r\") as fp:\n",
        "  relation_data = json.load(fp)"
      ],
      "metadata": {
        "id": "-Cxt4-uhSP2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the test data\n",
        "with open(\"test_relation.txt\", \"r\") as fp:\n",
        "  test_data = json.load(fp)"
      ],
      "metadata": {
        "id": "qLqoQz4dteHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, val = train_test_split(relation_data, test_size=0.2, random_state=1234)"
      ],
      "metadata": {
        "id": "Ssov_Y_zKEGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, tolerance=5, min_delta=0):\n",
        "        self.tolerance = tolerance\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.min_validation_loss = np.inf\n",
        "\n",
        "    def early_stop(self, validation_loss):\n",
        "        if validation_loss < self.min_validation_loss:\n",
        "            self.min_validation_loss = validation_loss\n",
        "            self.counter = 0\n",
        "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.tolerance:\n",
        "                return True\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "FNCUcUhuxcRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9cDGPMkKE8j"
      },
      "source": [
        "Run training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GOVPC5OdKE8j",
        "outputId": "c93bacd2-c12b-4f96-ff91-c4eb0e2a78ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label tensor([5.])\n"
          ]
        }
      ],
      "source": [
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "EMBEDDING_DIM = 5\n",
        "HIDDEN_DIM = 64\n",
        "\n",
        "training_data = train\n",
        "validation_data = val\n",
        "\n",
        "\n",
        "word_to_ix = {}\n",
        "for item in training_data:\n",
        "    for word in item[1]:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "for item in validation_data:\n",
        "    for word in item[1]:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "for item in test_data:\n",
        "    for word in item[1]:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "\n",
        "relation_to_ix = {START_TAG: 0, STOP_TAG: 1,'Strength-Drug': 2, 'Form-Drug': 3, 'Route-Drug': 4, 'Frequency-Drug': 5, 'Reason-Drug': 6, 'ADE-Drug': 7,\n",
        "                  'Dosage-Drug': 8, 'Duration-Drug': 9}\n",
        "ix_to_relation = {0: START_TAG, 1: STOP_TAG, 2: 'Strength-Drug', 3: 'Form-Drug', 4: 'Route-Drug', 5: 'Frequency-Drug', 6: 'Reason-Drug', 7: 'ADE-Drug',\n",
        "                  8: 'Dosage-Drug', 9: 'Duration-Drug'}\n",
        "\n",
        "model = BiLSTM(len(word_to_ix), relation_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4) #you can change it to dynamic optimisers such as Adam\n",
        "early_stopping = EarlyStopping(tolerance=5, min_delta=0)\n",
        "train_loss = []\n",
        "validation_loss = []\n",
        "epoch_i = 0\n",
        "# Check predictions before training\n",
        "with torch.no_grad():\n",
        "    precheck_sent = prepare_sequence(training_data[0][1], word_to_ix)\n",
        "    precheck_tags = torch.tensor([relation_to_ix[training_data[0][2][0][2]]], dtype=torch.float)\n",
        "    print('label',precheck_tags)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make sure prepare_sequence from earlier in the LSTM section is loaded\n",
        "for i in tqdm(range(50)):\n",
        "    for key, sentence, relations in training_data:\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        #print(len(relations))\n",
        "        for head, tail, rel in relations:\n",
        "            # Step 1. Remember that Pytorch accumulates gradients.\n",
        "            # We need to clear them out before each instance\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Step 2. Get our inputs ready for the network, that is,\n",
        "            # turn them into Tensors of word indices.\n",
        "\n",
        "            targets = torch.tensor(relation_to_ix[rel], dtype=torch.long)\n",
        "\n",
        "            # Step 3. Run our forward pass.\n",
        "            loss = model.cross_entropy_loss(sentence_in, head, tail, targets)\n",
        "            #print(loss)\n",
        "\n",
        "            # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "            # calling optimizer.step()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    train_loss.append(loss.item())\n",
        "    with torch.no_grad():\n",
        "      for key, sentence, tags in validation_data:\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        #print(len(relations))\n",
        "        for head, tail, rel in relations:\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\n",
        "          # We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Step 2. Get our inputs ready for the network, that is,\n",
        "          # turn them into Tensors of word indices.\n",
        "          targets = torch.tensor(relation_to_ix[rel], dtype=torch.long, requires_grad=False)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          val_loss = model.cross_entropy_loss(sentence_in, head, tail, targets)\n",
        "          #print(val_loss)\n",
        "\n",
        "    validation_loss.append(val_loss.item())\n",
        "    epoch_i += 1\n",
        "    # early stopping\n",
        "    if early_stopping.early_stop(val_loss):\n",
        "      print(\"We are at epoch:\", epoch_i)\n",
        "      break\n",
        "    validation_loss.append(val_loss.item())"
      ],
      "metadata": {
        "id": "yUu_Z_PgRyIy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4836de8b-494d-41ff-80e8-20be911f400d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 15/50 [23:41<55:17, 94.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We are at epoch: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check predictions\n",
        "with torch.no_grad():\n",
        "    key, sentence, relations = training_data[0]\n",
        "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "    for head, tail, _ in relations:\n",
        "        param = sentence_in, head, tail\n",
        "        print('prediction: ', ix_to_relation[np.argmax(model(*param).numpy())])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HltJyYRgIFpi",
        "outputId": "dde59172-6ace-4db1-a543-dd990dac293a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "prediction:  Frequency-Drug\n",
            "prediction:  Strength-Drug\n",
            "prediction:  Form-Drug\n",
            "prediction:  Dosage-Drug\n",
            "prediction:  Form-Drug\n",
            "prediction:  Route-Drug\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Individual RE model performance measure\n",
        "pred = []\n",
        "true = []\n",
        "for key, sentence, relations in test_data:\n",
        "  with torch.no_grad():\n",
        "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "    pred_relations = []\n",
        "    for head, tail, _ in relations:\n",
        "        param = sentence_in, head, tail\n",
        "        y_pred = np.argmax(model(*param).numpy())\n",
        "        pred.append(y_pred)\n",
        "    true_relations = []\n",
        "    for j in relations:\n",
        "        true.append(relation_to_ix[j[2]])\n",
        "print(f1_score(true, pred, average='weighted'))\n",
        "class_names = ['Strength-Drug', 'Form-Drug', 'Route-Drug', 'Frequency-Drug', 'Reason-Drug', 'ADE-Drug',\n",
        "                  'Dosage-Drug', 'Duration-Drug']\n",
        "print(classification_report(true, pred, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aE3JbzVswRo",
        "outputId": "4cff3b5e-ebe0-493b-c1c0-fcb82252f17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9425704313809986\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            " Strength-Drug       0.96      0.96      0.96      4211\n",
            "     Form-Drug       0.95      0.96      0.96      4304\n",
            "    Route-Drug       0.98      0.94      0.96      3503\n",
            "Frequency-Drug       0.98      0.98      0.98      3961\n",
            "   Reason-Drug       0.83      0.96      0.89      2442\n",
            "      ADE-Drug       0.80      0.50      0.61       607\n",
            "   Dosage-Drug       0.95      0.94      0.95      2657\n",
            " Duration-Drug       0.93      0.84      0.88       393\n",
            "\n",
            "      accuracy                           0.94     22078\n",
            "     macro avg       0.92      0.89      0.90     22078\n",
            "  weighted avg       0.94      0.94      0.94     22078\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(\"re_test\", \"rb\") as fp:\n",
        "  test_data_re = pickle.load(fp)"
      ],
      "metadata": {
        "id": "GKh1v1C2Vd_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data_re[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBtnALDPVpbb",
        "outputId": "c4d540f2-84c2-4d45-c8c5-daeeaa31a81f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['0-1', ['He', 'received', '2mg', 'IV', 'ativan', 'with', 'somnolence', 'and', 'apenea'], [[[6, 7], [4, 5], 'ADE-Drug'], [[8, 9], [4, 5], 'ADE-Drug'], [[3, 4], [4, 5], 'Route-Drug'], [[2, 3], [4, 5], 'Strength-Drug']]], ['0-6', ['Patient', 'developed', 'SVT', 'in', 'the', 'cath', 'lab', 'and', 'was', 'treated', 'with', '9', 'mg', 'IV', 'metoprolol'], [[[2, 3], [14, 15], 'Reason-Drug'], [[13, 14], [14, 15], 'Route-Drug'], [[11, 13], [14, 15], 'Strength-Drug']]], ['0-11', ['Evidence', 'of', 'right', 'heart', 'strain,', 'manifest', 'as', 'reflux', 'of', 'IV', 'contrast'], [[[9, 10], [10, 11], 'Route-Drug']]], ['0-15', ['He', 'was', 'started', 'on', 'heparin', 'ggt', 'in', 'house', 'and', 'transitioned', 'to', 'an', 'anticoagulation', 'regimen', 'of', 'coumadin', ',', 'with', 'a', 'lovenox'], [[[5, 6], [4, 5], 'Route-Drug']]], ['0-22', ['He', 'was', 'maintained', 'on', 'a', 'valium', 'CIWA', 'and', 'klonipin', '1mg', 'TID'], [[[10, 11], [8, 9], 'Frequency-Drug'], [[9, 10], [8, 9], 'Strength-Drug']]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# end to end RE pipeline model performance measure\n",
        "e2e_pred = []\n",
        "e2e_true = []\n",
        "for key, sentence, relations in test_data_re:\n",
        "  with torch.no_grad():\n",
        "    sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "    pred_relations = []\n",
        "    for head, tail, _ in relations:\n",
        "        param = sentence_in, head, tail\n",
        "        y_pred = np.argmax(model(*param).numpy())\n",
        "        e2e_pred.append(y_pred)\n",
        "    true_relations = []\n",
        "    for j in relations:\n",
        "        e2e_true.append(relation_to_ix[j[2]])\n",
        "print(f1_score(e2e_true, e2e_pred, average='weighted'))\n",
        "class_names = ['Strength-Drug', 'Form-Drug', 'Route-Drug', 'Frequency-Drug', 'Reason-Drug', 'ADE-Drug',\n",
        "                  'Dosage-Drug', 'Duration-Drug']\n",
        "print(classification_report(e2e_true, e2e_pred, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWQuA4yVVi5t",
        "outputId": "352ec76d-db8f-413d-c11a-fd0072dbe4c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9854062611625906\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            " Strength-Drug       0.98      0.99      0.98      2961\n",
            "     Form-Drug       0.99      1.00      0.99      3342\n",
            "    Route-Drug       0.99      0.99      0.99      2700\n",
            "Frequency-Drug       0.99      0.99      0.99      2612\n",
            "   Reason-Drug       0.96      0.98      0.97      1270\n",
            "      ADE-Drug       0.84      0.76      0.80       150\n",
            "   Dosage-Drug       0.99      0.97      0.98      1993\n",
            " Duration-Drug       0.99      0.94      0.96       215\n",
            "\n",
            "      accuracy                           0.99     15243\n",
            "     macro avg       0.97      0.95      0.96     15243\n",
            "  weighted avg       0.99      0.99      0.99     15243\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hypermarameter tuning\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DAiKVvExe5dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "1. optimiser: SGD, lr = 0.01, HIDDEN_DIM = 64, tolerance=5\n",
        "  weighted avg f1: 0.90, 13 epochs\n",
        "1. optimiser: SGD, lr = 0.01, HIDDEN_DIM = 64, tolerance=5\n",
        "  weighted avg f1: 0.9063, 7 epochs\n",
        "2. optimiser: SGD, lr = 0.1, HIDDEN_DIM = 64, tolerance=5\n",
        "  weighted avg f1: 0.9485, 8 epochs\n",
        "2. optimiser: SGD, lr = 0.1, HIDDEN_DIM = 64, tolerance=5\n",
        "  weighted avg f1: 0.9538, 21 epochs\n",
        "2. optimiser: SGD, lr = 0.1, HIDDEN_DIM = 64, tolerance=5\n",
        "  weighted avg f1: 0.9471, 8 epochs\n",
        "3. optimiser: SGD, lr = 0.1, HIDDEN_DIM = 16, tolerance=5\n",
        "  weighted avg f1: 0.9499, 9 epochs\n",
        "3. optimiser: SGD, lr = 0.1, HIDDEN_DIM = 16, tolerance=5\n",
        "  weighted avg f1: 0.9497, 11 epochs\n",
        "4. optimiser: Adam, lr = 0.1, HIDDEN_DIM = 16, tolerance=5\n",
        "  weighted avg f1: 0.64, 10 epochs\n",
        "5. optimiser: Adam, lr = 0.1, HIDDEN_DIM = 64, tolerance=5\n",
        "  weighted avg f1: 0.5843, 11 epochs\n",
        "6. optimiser: SGD, lr = 1, HIDDEN_DIM = 64, tolerance=5\n",
        "  weighted avg f1: 0.5168, 6 epochs\n",
        "7. optimiser: SGD, lr = 0.1, HIDDEN_DIM = 32, tolerance=5\n",
        "  weighted avg f1: 0.9494, 12 epochs\n",
        "8. optimiser: SGD, lr = 0.1, HIDDEN_DIM = 128, tolerance=5\n",
        "  weighted avg f1: 0.9531, 26 epochs"
      ],
      "metadata": {
        "id": "sZ8c1zUohOxy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}